class FL_SliderLoraAdvConfig:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "learning_rate": ("FLOAT", {"default": 1e-4, "min": 1e-6, "max": 1e-2, "step": 1e-6}),
                "num_train_epochs": ("INT", {"default": 50, "min": 1, "max": 1000}),
                "max_train_steps": ("INT", {"default": 10000, "min": 100, "max": 1000000}),
                "gradient_accumulation_steps": ("INT", {"default": 1, "min": 1, "max": 64}),
                "lr_scheduler": (["constant", "linear", "cosine", "cosine_with_restarts"], {"default": "constant"}),
                "lr_warmup_steps": ("INT", {"default": 0, "min": 0, "max": 10000}),
                "adam_beta1": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01}),
                "adam_beta2": ("FLOAT", {"default": 0.999, "min": 0.0, "max": 1.0, "step": 0.001}),
                "adam_weight_decay": ("FLOAT", {"default": 1e-2, "min": 0.0, "max": 1.0, "step": 1e-3}),
                "adam_epsilon": ("FLOAT", {"default": 1e-8, "min": 1e-10, "max": 1e-6, "step": 1e-10}),
                "max_grad_norm": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 10.0, "step": 0.1}),
            },
            "optional": {
                "train_text_encoder": ("BOOLEAN", {"default": False}),
                "predenoise_num_train_timesteps": ("INT", {"default": 50, "min": 1, "max": 1000}),
                "noise_offset": ("FLOAT", {"default": 0.0, "min": 0.0, "max": 1.0, "step": 0.01}),
                "use_8bit_adam": ("BOOLEAN", {"default": False}),
                "gradient_checkpointing": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("FL_SLIDER_LORA_ADV_CONFIG",)
    RETURN_NAMES = ("advanced_config",)

    FUNCTION = "create_advanced_config"
    CATEGORY = "FL_Slider_Lora"

    def create_advanced_config(self, learning_rate, num_train_epochs, max_train_steps, gradient_accumulation_steps,
                               lr_scheduler, lr_warmup_steps, adam_beta1, adam_beta2, adam_weight_decay, adam_epsilon,
                               max_grad_norm, train_text_encoder=False, predenoise_num_train_timesteps=50,
                               noise_offset=0.0, use_8bit_adam=False, gradient_checkpointing=False):
        advanced_config = {
            "learning_rate": learning_rate,
            "num_train_epochs": num_train_epochs,
            "max_train_steps": max_train_steps,
            "gradient_accumulation_steps": gradient_accumulation_steps,
            "lr_scheduler": lr_scheduler,
            "lr_warmup_steps": lr_warmup_steps,
            "adam_beta1": adam_beta1,
            "adam_beta2": adam_beta2,
            "adam_weight_decay": adam_weight_decay,
            "adam_epsilon": adam_epsilon,
            "max_grad_norm": max_grad_norm,
            "train_text_encoder": train_text_encoder,
            "predenoise_num_train_timesteps": predenoise_num_train_timesteps,
            "noise_offset": noise_offset,
            "use_8bit_adam": use_8bit_adam,
            "gradient_checkpointing": gradient_checkpointing,
        }

        return (advanced_config,)